{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshata1712/Federated-Learning-/blob/main/FedML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQYzALrmS0-R",
        "outputId": "2ad50fe3-45cf-4a32-ea21-ec3b4a6734d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installs (TabNet + XGBoost). This may take ~1-2 minutes.\n",
        "!pip install -q xgboost pytorch-tabnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tSI1fXgTxwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05ecf03-e72f-4642-dc49-3504ad228481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e7a3fd22a10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update this to your creditcard.csv location in Drive\n",
        "DATA_PATH = \"/content/drive/MyDrive/creditcard.csv\"\n",
        "\n",
        "# Standard imports\n",
        "import os, random, time\n",
        "import numpy as np, pandas as pd\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For TabNet / XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Device + seed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBXnuVnUacaM",
        "outputId": "c315fbd6-d728-41f7-c23d-823755deecfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.0.1 Requires-Python >=2.6,<=3.0; 0.0.2 Requires-Python >=2.6,<=3.0; 0.0.3 Requires-Python >=2.6,<=3.0; 0.0.4 Requires-Python >=2.6,<=3.0; 0.0.5 Requires-Python >=2.6,<=3.0; 0.0.6 Requires-Python >=2.6,<=3.0; 0.0.7 Requires-Python >=2.6,<=3.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement data_utils (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for data_utils\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install data_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru6w42p0a7-H",
        "outputId": "fa673df8-a044-4351-bd94-c4834a78a2d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes (torch tensors): torch.Size([227848, 30]) torch.Size([227848, 1]) torch.Size([28478, 30]) torch.Size([28481, 30])\n",
            "Train fraud rate: 0.001729222945868969\n"
          ]
        }
      ],
      "source": [
        "def load_and_preprocess(path=DATA_PATH, seed=SEED, test_size=0.10, distill_size=0.1111):\n",
        "    df = pd.read_csv(path)\n",
        "    X = df.drop(\"Class\", axis=1).values\n",
        "    y = df[\"Class\"].values.reshape(-1,1).astype(np.float32)\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y)\n",
        "    X_train, X_distill, y_train, y_distill = train_test_split(\n",
        "        X_train_full, y_train_full, test_size=distill_size, random_state=seed, stratify=y_train_full)\n",
        "    # to tensors (torch) for torch models; for non-torch keep numpy copies too\n",
        "    X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "    X_distill_t = torch.tensor(X_distill, dtype=torch.float32).to(device)\n",
        "    y_distill_t = torch.tensor(y_distill, dtype=torch.float32).to(device)\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_t = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "    # also provide numpy versions for non-torch models\n",
        "    return X_train_t, y_train_t, X_distill_t, y_distill_t, X_test_t, y_test_t, X_train, y_train, X_distill, y_distill, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_distill, y_distill, X_test, y_test, \\\n",
        "X_train_np, y_train_np, X_distill_np, y_distill_np, X_test_np, y_test_np = load_and_preprocess()\n",
        "print(\"Shapes (torch tensors):\", X_train.shape, y_train.shape, X_distill.shape, X_test.shape)\n",
        "print(\"Train fraud rate:\", float((y_train==1).float().mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-iewqgXanKQ"
      },
      "outputs": [],
      "source": [
        "# Torch models (logits output)\n",
        "class StdMLP(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(dim,64), nn.ReLU(), nn.Linear(64,1))\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(dim,64), nn.ReLU(),\n",
        "                                 nn.Linear(64,64), nn.ReLU(),\n",
        "                                 nn.Linear(64,1))\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class WideMLP(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(dim,128), nn.ReLU(), nn.Linear(128,1))\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Wrappers for non-torch models to produce probabilities on numpy arrays\n",
        "class XGBWrap:\n",
        "    def __init__(self, scale_pos_weight=1.0):\n",
        "        self.model = XGBClassifier(scale_pos_weight=float(scale_pos_weight), use_label_encoder=False, eval_metric='logloss', n_jobs=1)\n",
        "    def fit(self, X_np, y_np):\n",
        "        self.model.fit(X_np, y_np.ravel())\n",
        "    def predict_proba(self, X_np):\n",
        "        # returns column vector of probabilities shape (N,1)\n",
        "        return self.model.predict_proba(X_np)[:, 1].reshape(-1,1)\n",
        "\n",
        "class TabNetWrap:\n",
        "    def __init__(self):\n",
        "        # small TabNet config (train short for speed)\n",
        "        self.model = TabNetClassifier(n_d=16, n_a=16, n_steps=3, verbose=0)\n",
        "    def fit(self, X_np, y_np):\n",
        "        # TabNet expects 2D numpy arrays\n",
        "        self.model.fit(X_np, y_np.ravel(), max_epochs=10, batch_size=256, virtual_batch_size=64, patience=5)\n",
        "    def predict_proba(self, X_np):\n",
        "        return self.model.predict_proba(X_np)[:,1].reshape(-1,1)\n",
        "\n",
        "class RFWrap:\n",
        "    def __init__(self):\n",
        "        self.model = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=SEED)\n",
        "    def fit(self, X_np, y_np):\n",
        "        self.model.fit(X_np, y_np.ravel())\n",
        "    def predict_proba(self, X_np):\n",
        "        return self.model.predict_proba(X_np)[:,1].reshape(-1,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QytVD_4LT7iS"
      },
      "outputs": [],
      "source": [
        "# Local training for torch models with local validation split; returns model & local val F1\n",
        "def local_train_and_eval_torch(model, X_t, y_t, epochs=1, batch_size=64, lr=0.01, pos_weight=None, val_frac=0.1):\n",
        "    n = X_t.shape[0]\n",
        "    if n < 4:\n",
        "        X_tr, X_val = X_t, X_t\n",
        "        y_tr, y_val = y_t, y_t\n",
        "    else:\n",
        "        val_n = max(1, int(val_frac * n))\n",
        "        X_tr, X_val = X_t[:-val_n], X_t[-val_n:]\n",
        "        y_tr, y_val = y_t[:-val_n], y_t[-val_n:]\n",
        "\n",
        "    ds = TensorDataset(X_tr, y_tr)\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    model.train()\n",
        "    if pos_weight is not None:\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
        "    else:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "    opt = optim.SGD(model.parameters(), lr=lr)\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "    # val f1\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = torch.sigmoid(model(X_val))\n",
        "        preds = (probs > 0.5).float()\n",
        "    val_f1 = f1_score(y_val.cpu().numpy(), preds.cpu().numpy(), zero_division=0)\n",
        "    return model, val_f1\n",
        "\n",
        "# Training non-torch model wrapper with local validation: returns fitted model and val_f1\n",
        "def local_train_and_eval_non_torch(wrapper, X_np, y_np, val_frac=0.1):\n",
        "    n = X_np.shape[0]\n",
        "    if n < 4:\n",
        "        X_tr, X_val = X_np, X_np\n",
        "        y_tr, y_val = y_np, y_np\n",
        "    else:\n",
        "        val_n = max(1, int(val_frac * n))\n",
        "        X_tr, X_val = X_np[:-val_n], X_np[-val_n:]\n",
        "        y_tr, y_val = y_np[:-val_n], y_np[-val_n:]\n",
        "    wrapper.fit(X_tr, y_tr)\n",
        "    # predict on validation\n",
        "    probs = wrapper.predict_proba(X_val)\n",
        "    preds = (probs > 0.5).astype(float)\n",
        "    val_f1 = f1_score(y_val.ravel(), preds.ravel(), zero_division=0)\n",
        "    return wrapper, val_f1\n",
        "\n",
        "# Evaluate a logits model (torch) on test\n",
        "def evaluate_logits(model, X_t, y_t):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = torch.sigmoid(model(X_t))\n",
        "        preds = (probs > 0.5).float()\n",
        "    acc = accuracy_score(y_t.cpu().numpy(), preds.cpu().numpy())\n",
        "    f1 = f1_score(y_t.cpu().numpy(), preds.cpu().numpy(), zero_division=0)\n",
        "    bce = nn.BCELoss()\n",
        "    with torch.no_grad():\n",
        "        loss = bce(probs, y_t).item()\n",
        "    return acc, f1, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_splits(n_samples, num_clients):\n",
        "    return [list(range(i, n_samples, num_clients)) for i in range(num_clients)]\n",
        "\n",
        "def capacity_scores(models):\n",
        "    # models: list of objects (torch models or wrappers). For wrappers that are not torch, we assign approximate capacity via hasattr('model') and .n_estimators etc.\n",
        "    caps = []\n",
        "    for m in models:\n",
        "        if isinstance(m, nn.Module):\n",
        "            caps.append(count_params(m))\n",
        "        else:\n",
        "            # heuristic capacities for classical models\n",
        "            if isinstance(m, XGBWrap):\n",
        "                # approximate capacity: number of trees * depth? can't access easily; use constant\n",
        "                caps.append(10000)  # heuristic\n",
        "            elif isinstance(m, TabNetWrap):\n",
        "                caps.append(20000)\n",
        "            elif isinstance(m, RFWrap):\n",
        "                caps.append(15000)\n",
        "            else:\n",
        "                caps.append(10000)\n",
        "    caps = np.array(caps, dtype=float)\n",
        "    # log-normalize to 0..1\n",
        "    logp = np.log(caps + 1.0)\n",
        "    norm = (logp - logp.min()) / (logp.max() - logp.min() + 1e-12)\n",
        "    return norm\n"
      ],
      "metadata": {
        "id": "BxTSJGm9yog2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_homogeneous_fedavg(X_train_t, y_train_t, X_test_t, y_test_t,\n",
        "                           num_clients=10, rounds=20, local_epochs=1, lr=0.01, batch_size=64):\n",
        "    n = X_train_t.shape[0]\n",
        "    splits = create_splits(n, num_clients)\n",
        "    client_sizes = [len(s) for s in splits]\n",
        "    pos_weight = (y_train_t==0).sum() / (y_train_t==1).sum()\n",
        "    dim = X_train_t.shape[1]\n",
        "    global_model = StdMLP(dim).to(device)\n",
        "    accs, f1s, losses = [], [], []\n",
        "    for r in range(rounds):\n",
        "        local_states = []\n",
        "        for i in range(num_clients):\n",
        "            idx = splits[i]\n",
        "            Xc, yc = X_train_t[idx], y_train_t[idx]\n",
        "            client = StdMLP(dim).to(device)\n",
        "            client.load_state_dict(global_model.state_dict())\n",
        "            client, _ = local_train_and_eval_torch(client, Xc, yc, epochs=local_epochs, batch_size=batch_size, lr=lr, pos_weight=pos_weight)\n",
        "            local_states.append({k: v.cpu() for k,v in client.state_dict().items()})\n",
        "        # FedAvg\n",
        "        total = sum(client_sizes)\n",
        "        new_state = {}\n",
        "        for k in local_states[0].keys():\n",
        "            new_state[k] = sum(local_states[i][k] * client_sizes[i] for i in range(num_clients)) / total\n",
        "        global_model.load_state_dict(new_state)\n",
        "        acc, f1, loss = evaluate_logits(global_model, X_test_t, y_test_t)\n",
        "        accs.append(acc); f1s.append(f1); losses.append(loss)\n",
        "        print(f\"[FedAvg] Round {r+1}/{rounds}: Acc={acc:.4f}, F1={f1:.4f}, BCE(test)={loss:.4f}\")\n",
        "    return accs, f1s, losses, global_model\n",
        "\n",
        "# Quick run (adjust rounds/local_epochs as desired)\n",
        "hom_accs, hom_f1s, hom_losses, hom_global = run_homogeneous_fedavg(X_train, y_train, X_test, y_test,\n",
        "                                                                  num_clients=10, rounds=10, local_epochs=1)\n"
      ],
      "metadata": {
        "id": "R5pP8QugZdxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d146f198-3bc0-42b5-f2eb-8c19b909f713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FedAvg] Round 1/10: Acc=0.9762, F1=0.1147, BCE(test)=0.3111\n",
            "[FedAvg] Round 2/10: Acc=0.9803, F1=0.1356, BCE(test)=0.1761\n",
            "[FedAvg] Round 3/10: Acc=0.9825, F1=0.1504, BCE(test)=0.1382\n",
            "[FedAvg] Round 4/10: Acc=0.9841, F1=0.1661, BCE(test)=0.1196\n",
            "[FedAvg] Round 5/10: Acc=0.9754, F1=0.1139, BCE(test)=0.1394\n",
            "[FedAvg] Round 6/10: Acc=0.9787, F1=0.1289, BCE(test)=0.1245\n",
            "[FedAvg] Round 7/10: Acc=0.9836, F1=0.1616, BCE(test)=0.1056\n",
            "[FedAvg] Round 8/10: Acc=0.9781, F1=0.1261, BCE(test)=0.1177\n",
            "[FedAvg] Round 9/10: Acc=0.9818, F1=0.1478, BCE(test)=0.1073\n",
            "[FedAvg] Round 10/10: Acc=0.9833, F1=0.1593, BCE(test)=0.1014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_hetero_mlp_distill(X_train_t, y_train_t, X_distill_t, y_distill_t, X_test_t, y_test_t,\n",
        "                           client_archs, num_clients=10, rounds=20, local_epochs=1, lr=0.01, batch_size=64,\n",
        "                           alpha=0.7, beta=0.3):\n",
        "    \"\"\"\n",
        "    client_archs: list of length num_clients containing 'std'/'deep'/'wide'\n",
        "    alpha,beta: weight for val_f1 and capacity in capacity-aware weighting\n",
        "    \"\"\"\n",
        "    n = X_train_t.shape[0]\n",
        "    splits = create_splits(n, num_clients)\n",
        "    pos_weight = (y_train_t==0).sum() / (y_train_t==1).sum()\n",
        "    dim = X_train_t.shape[1]\n",
        "\n",
        "    # instantiate clients\n",
        "    clients = []\n",
        "    for arch in client_archs:\n",
        "        if arch == 'std': clients.append(StdMLP(dim).to(device))\n",
        "        elif arch == 'deep': clients.append(DeepMLP(dim).to(device))\n",
        "        elif arch == 'wide': clients.append(WideMLP(dim).to(device))\n",
        "        else: raise ValueError(\"unknown arch\")\n",
        "\n",
        "    # capacity norm\n",
        "    cap_norm = capacity_scores(clients)  # 0..1 numpy\n",
        "\n",
        "    accs_simple, f1s_simple, losses_simple = [], [], []\n",
        "    accs_cap, f1s_cap, losses_cap = [], [], []\n",
        "\n",
        "    for r in range(rounds):\n",
        "        val_f1s = []\n",
        "        # local train each client and get val_f1\n",
        "        for i, m in enumerate(clients):\n",
        "            idx = splits[i]\n",
        "            Xc, yc = X_train_t[idx], y_train_t[idx]\n",
        "            m, valf1 = local_train_and_eval_torch(m, Xc, yc, epochs=local_epochs, batch_size=batch_size, lr=lr, pos_weight=pos_weight)\n",
        "            val_f1s.append(valf1)\n",
        "        val_f1s = np.array(val_f1s, dtype=float)\n",
        "        # --- Simple distillation: average probs equally ---\n",
        "        with torch.no_grad():\n",
        "            preds_sum = None\n",
        "            for m in clients:\n",
        "                out = torch.sigmoid(m(X_distill_t))   # shape [D,1]\n",
        "                preds_sum = out.clone() if preds_sum is None else preds_sum + out\n",
        "            avg_soft_simple = (preds_sum / len(clients)).clamp(0.0,1.0)\n",
        "\n",
        "        # distill into global model (simple)\n",
        "        global_simple = StdMLP(dim).to(device)\n",
        "        optg = optim.SGD(global_simple.parameters(), lr=lr)\n",
        "        bce = nn.BCELoss()\n",
        "        for _ in range(local_epochs):\n",
        "            optg.zero_grad()\n",
        "            gp = torch.sigmoid(global_simple(X_distill_t))\n",
        "            loss = bce(gp, avg_soft_simple)\n",
        "            loss.backward(); optg.step()\n",
        "        acc_s, f1_s, loss_s = evaluate_logits(global_simple, X_test_t, y_test_t)\n",
        "        accs_simple.append(acc_s); f1s_simple.append(f1_s); losses_simple.append(loss_s)\n",
        "\n",
        "        # --- Capacity-weighted distillation: weights from val_f1 and cap_norm ---\n",
        "        scores = alpha * (val_f1s - val_f1s.min()) / (val_f1s.max()-val_f1s.min()+1e-12) + beta * cap_norm\n",
        "        # normalize to sum=1\n",
        "        if scores.sum() == 0:\n",
        "            weights = np.ones_like(scores)/len(scores)\n",
        "        else:\n",
        "            weights = scores / scores.sum()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds_weighted = None\n",
        "            for m, w in zip(clients, weights):\n",
        "                out = torch.sigmoid(m(X_distill_t))\n",
        "                if preds_weighted is None:\n",
        "                    preds_weighted = w * out\n",
        "                else:\n",
        "                    preds_weighted += w * out\n",
        "            avg_soft_cap = preds_weighted.clamp(0.0,1.0)\n",
        "\n",
        "        # distill into global model (capacity-weighted)\n",
        "        global_cap = StdMLP(dim).to(device)\n",
        "        optg2 = optim.SGD(global_cap.parameters(), lr=lr)\n",
        "        for _ in range(local_epochs):\n",
        "            optg2.zero_grad()\n",
        "            gp2 = torch.sigmoid(global_cap(X_distill_t))\n",
        "            loss2 = bce(gp2, avg_soft_cap)\n",
        "            loss2.backward(); optg2.step()\n",
        "        acc_c, f1_c, loss_c = evaluate_logits(global_cap, X_test_t, y_test_t)\n",
        "        accs_cap.append(acc_c); f1s_cap.append(f1_c); losses_cap.append(loss_c)\n",
        "\n",
        "        print(f\"[Hetero-MLP] Round {r+1}/{rounds}: Simple Acc={acc_s:.4f}, F1={f1_s:.4f} | Cap-Weighted Acc={acc_c:.4f}, F1={f1_c:.4f}\")\n",
        "\n",
        "    return (accs_simple, f1s_simple, losses_simple, accs_cap, f1s_cap, losses_cap)\n",
        "\n",
        "# Example client mix\n",
        "client_archs = ['std']*4 + ['deep']*3 + ['wide']*3\n",
        "hetmlp_results = run_hetero_mlp_distill(X_train, y_train, X_distill, y_distill, X_test, y_test,\n",
        "                                        client_archs, num_clients=10, rounds=10, local_epochs=1)\n"
      ],
      "metadata": {
        "id": "FE40TY6GZeiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829ba847-1037-46d3-dc61-4998ccce6025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hetero-MLP] Round 1/10: Simple Acc=0.3666, F1=0.0050 | Cap-Weighted Acc=0.9601, F1=0.0018\n",
            "[Hetero-MLP] Round 2/10: Simple Acc=0.4723, F1=0.0058 | Cap-Weighted Acc=0.8167, F1=0.0173\n",
            "[Hetero-MLP] Round 3/10: Simple Acc=0.4105, F1=0.0047 | Cap-Weighted Acc=0.8108, F1=0.0096\n",
            "[Hetero-MLP] Round 4/10: Simple Acc=0.1107, F1=0.0036 | Cap-Weighted Acc=0.9683, F1=0.0066\n",
            "[Hetero-MLP] Round 5/10: Simple Acc=0.0784, F1=0.0036 | Cap-Weighted Acc=0.1648, F1=0.0038\n",
            "[Hetero-MLP] Round 6/10: Simple Acc=0.4416, F1=0.0045 | Cap-Weighted Acc=0.7673, F1=0.0018\n",
            "[Hetero-MLP] Round 7/10: Simple Acc=0.0047, F1=0.0034 | Cap-Weighted Acc=0.3652, F1=0.0028\n",
            "[Hetero-MLP] Round 8/10: Simple Acc=0.5579, F1=0.0030 | Cap-Weighted Acc=0.7003, F1=0.0084\n",
            "[Hetero-MLP] Round 9/10: Simple Acc=0.4804, F1=0.0030 | Cap-Weighted Acc=0.9340, F1=0.0042\n",
            "[Hetero-MLP] Round 10/10: Simple Acc=0.7918, F1=0.0003 | Cap-Weighted Acc=0.5772, F1=0.0022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_hetero_finance_distill(X_train_t, y_train_t, X_distill_t, y_distill_t, X_test_t, y_test_t,\n",
        "                               X_train_np, y_train_np, X_distill_np, y_distill_np, X_test_np, y_test_np,\n",
        "                               client_wrappers, num_clients=3, rounds=20, local_epochs=5, lr=0.01,\n",
        "                               alpha=0.7, beta=0.3):\n",
        "    \"\"\"\n",
        "    client_wrappers: list of classes e.g. [XGBWrap, TabNetWrap, RFWrap]\n",
        "    This creates num_clients=len(client_wrappers) clients.\n",
        "    \"\"\"\n",
        "    assert len(client_wrappers) == num_clients\n",
        "    n = X_train_t.shape[0]\n",
        "    splits = create_splits(n, num_clients)\n",
        "    pos_weight = (y_train_t==0).sum() / (y_train_t==1).sum()\n",
        "    dim = X_train_t.shape[1]\n",
        "\n",
        "    # instantiate wrappers\n",
        "    clients = []\n",
        "    for W in client_wrappers:\n",
        "        if W == XGBWrap:\n",
        "            clients.append(XGBWrap(scale_pos_weight=float(pos_weight.cpu().item())))\n",
        "        elif W == TabNetWrap:\n",
        "            clients.append(TabNetWrap())\n",
        "        elif W == RFWrap:\n",
        "            clients.append(RFWrap())\n",
        "        else:\n",
        "            clients.append(W())\n",
        "\n",
        "    # compute capacities (heuristic) and normalize\n",
        "    cap_norm = capacity_scores(clients)\n",
        "\n",
        "    accs_simple, f1s_simple, losses_simple = [], [], []\n",
        "    accs_cap, f1s_cap, losses_cap = [], [], []\n",
        "\n",
        "    # Initialize global models once before rounds start\n",
        "    global_simple = StdMLP(dim).to(device)\n",
        "    global_cap = StdMLP(dim).to(device)\n",
        "    bce = nn.BCELoss()\n",
        "\n",
        "    for r in range(rounds):\n",
        "        val_f1s = []\n",
        "        # Local training of each wrapper on its split\n",
        "        for i, wrapper in enumerate(clients):\n",
        "            idx = splits[i]\n",
        "            Xc_np, yc_np = X_train_np[idx], y_train_np[idx]\n",
        "            wrapper, valf1 = local_train_and_eval_non_torch(wrapper, Xc_np, yc_np)\n",
        "            val_f1s.append(valf1)\n",
        "        val_f1s = np.array(val_f1s, dtype=float)\n",
        "\n",
        "        # --- Simple averaging of soft labels ---\n",
        "        preds_sum = None\n",
        "        for wrapper in clients:\n",
        "            p = wrapper.predict_proba(X_distill_np)  # numpy (D,1)\n",
        "            if preds_sum is None:\n",
        "                preds_sum = p.copy()\n",
        "            else:\n",
        "                preds_sum += p\n",
        "        avg_soft_simple = torch.tensor((preds_sum / len(clients)).astype(np.float32)).to(device).clamp(0.0,1.0)\n",
        "\n",
        "        # Distill into global simple model\n",
        "        optg = optim.SGD(global_simple.parameters(), lr=lr)\n",
        "        for _ in range(local_epochs):\n",
        "            optg.zero_grad()\n",
        "            gp = torch.sigmoid(global_simple(X_distill_t))\n",
        "            loss = bce(gp, avg_soft_simple)\n",
        "            loss.backward()\n",
        "            optg.step()\n",
        "        acc_s, f1_s, loss_s = evaluate_logits(global_simple, X_test_t, y_test_t)\n",
        "        accs_simple.append(acc_s)\n",
        "        f1s_simple.append(f1_s)\n",
        "        losses_simple.append(loss_s)\n",
        "\n",
        "        # --- Capacity-aware weighted averaging ---\n",
        "        scores = alpha * (val_f1s - val_f1s.min()) / (val_f1s.max() - val_f1s.min() + 1e-12) + beta * cap_norm\n",
        "        if scores.sum() == 0:\n",
        "            weights = np.ones_like(scores) / len(scores)\n",
        "        else:\n",
        "            weights = scores / scores.sum()\n",
        "        preds_w = None\n",
        "        for wrapper, w in zip(clients, weights):\n",
        "            p = wrapper.predict_proba(X_distill_np)\n",
        "            if preds_w is None:\n",
        "                preds_w = w * p.copy()\n",
        "            else:\n",
        "                preds_w += w * p.copy()\n",
        "        avg_soft_cap = torch.tensor(preds_w.astype(np.float32)).to(device).clamp(0.0,1.0)\n",
        "\n",
        "        # Distill into global capacity-weighted model\n",
        "        optg2 = optim.SGD(global_cap.parameters(), lr=lr)\n",
        "        for _ in range(local_epochs):\n",
        "            optg2.zero_grad()\n",
        "            gp2 = torch.sigmoid(global_cap(X_distill_t))\n",
        "            loss2 = bce(gp2, avg_soft_cap)\n",
        "            loss2.backward()\n",
        "            optg2.step()\n",
        "        acc_c, f1_c, loss_c = evaluate_logits(global_cap, X_test_t, y_test_t)\n",
        "        accs_cap.append(acc_c)\n",
        "        f1s_cap.append(f1_c)\n",
        "        losses_cap.append(loss_c)\n",
        "\n",
        "        print(f\"[Hetero-Finance] Round {r+1}/{rounds}: Simple Acc={acc_s:.4f}, F1={f1_s:.4f} | Cap-Weighted Acc={acc_c:.4f}, F1={f1_c:.4f}\")\n",
        "\n",
        "    return (accs_simple, f1s_simple, losses_simple, accs_cap, f1s_cap, losses_cap)\n",
        "\n",
        "# Run the experiment\n",
        "finance_wrappers = [XGBWrap, TabNetWrap, RFWrap]\n",
        "hetfin_results = run_hetero_finance_distill(\n",
        "    X_train, y_train, X_distill, y_distill, X_test, y_test,\n",
        "    X_train_np, y_train_np, X_distill_np, y_distill_np, X_test_np, y_test_np,\n",
        "    finance_wrappers, num_clients=3, rounds=10, local_epochs=5, lr=0.01\n",
        ")\n"
      ],
      "metadata": {
        "id": "2q9tc2vhZhOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf456bed-6941-446c-8eef-bb8436d1be2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [05:57:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hetero-Finance] Round 1/10: Simple Acc=0.6329, F1=0.0006 | Cap-Weighted Acc=0.8085, F1=0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [05:59:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hetero-Finance] Round 2/10: Simple Acc=0.6329, F1=0.0006 | Cap-Weighted Acc=0.8085, F1=0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [06:01:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hetero-Finance] Round 3/10: Simple Acc=0.6329, F1=0.0006 | Cap-Weighted Acc=0.8085, F1=0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [06:04:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hetero-Finance] Round 4/10: Simple Acc=0.6329, F1=0.0006 | Cap-Weighted Acc=0.8085, F1=0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [06:06:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hetero-Finance] Round 5/10: Simple Acc=0.6329, F1=0.0006 | Cap-Weighted Acc=0.8085, F1=0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [06:08:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hetero-Finance] Round 6/10: Simple Acc=0.6329, F1=0.0006 | Cap-Weighted Acc=0.8085, F1=0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [06:11:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hetero-Finance] Round 7/10: Simple Acc=0.6329, F1=0.0006 | Cap-Weighted Acc=0.8085, F1=0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [06:13:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack results\n",
        "hom_rounds = len(hom_accs)\n",
        "hetmlp_accs_s, hetmlp_f1s_s, hetmlp_losses_s, hetmlp_accs_c, hetmlp_f1s_c, hetmlp_losses_c = hetmlp_results\n",
        "hetfin_accs_s, hetfin_f1s_s, hetfin_losses_s, hetfin_accs_c, hetfin_f1s_c, hetfin_losses_c = hetfin_results\n",
        "\n",
        "rounds = range(1, hom_rounds+1)\n",
        "\n",
        "plt.figure(figsize=(16,5))\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(rounds, hom_accs, label='Homog FedAvg')\n",
        "plt.plot(rounds, hetmlp_accs_s, label='Hetero-MLP (simple distill)')\n",
        "plt.plot(rounds, hetmlp_accs_c, label='Hetero-MLP (cap-weight distill)')\n",
        "plt.plot(range(1,len(hetfin_accs_s)+1), hetfin_accs_s, label='Hetero-Fin (simple)')\n",
        "plt.plot(range(1,len(hetfin_accs_c)+1), hetfin_accs_c, label='Hetero-Fin (cap-weight)')\n",
        "plt.xlabel(\"Round\"); plt.ylabel(\"Test Accuracy\"); plt.title(\"Accuracy vs Rounds\"); plt.legend()\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(rounds, hom_f1s, label='Homog FedAvg')\n",
        "plt.plot(rounds, hetmlp_f1s_s, label='Hetero-MLP (simple)')\n",
        "plt.plot(rounds, hetmlp_f1s_c, label='Hetero-MLP (cap)')\n",
        "plt.plot(range(1,len(hetfin_f1s_s)+1), hetfin_f1s_s, label='Hetero-Fin (simple)')\n",
        "plt.plot(range(1,len(hetfin_f1s_c)+1), hetfin_f1s_c, label='Hetero-Fin (cap)')\n",
        "plt.xlabel(\"Round\"); plt.ylabel(\"Test F1\"); plt.title(\"F1 vs Rounds\"); plt.legend()\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(rounds, hom_losses, label='Homog FedAvg')\n",
        "plt.plot(rounds, hetmlp_losses_s, label='Hetero-MLP (simple)')\n",
        "plt.plot(rounds, hetmlp_losses_c, label='Hetero-MLP (cap)')\n",
        "plt.plot(range(1,len(hetfin_losses_s)+1), hetfin_losses_s, label='Hetero-Fin (simple)')\n",
        "plt.plot(range(1,len(hetfin_losses_c)+1), hetfin_losses_c, label='Hetero-Fin (cap)')\n",
        "plt.xlabel(\"Round\"); plt.ylabel(\"BCE Loss (test)\"); plt.title(\"Test Loss vs Rounds\"); plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1ZjTpYH-Zj4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b8561a88-66a1-4777-e0be-40b4d40495a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hom_accs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2198365022.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unpack results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhom_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhom_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhetmlp_accs_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetmlp_f1s_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetmlp_losses_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetmlp_accs_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetmlp_f1s_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetmlp_losses_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhetmlp_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhetfin_accs_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetfin_f1s_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetfin_losses_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetfin_accs_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetfin_f1s_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhetfin_losses_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhetfin_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hom_accs' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzSf8q1V5fZHesfkjKJFrs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}